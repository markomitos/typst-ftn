= Теоријске основе

== Федеративно учење

Федеративно учење (_Federated Learning_, FL) је техника машинског учења која
омогућава децентрализовано тренирање модела над поверљивим подацима.
За разлику од традиционалног централизованог приступа, где се сви подаци
прикупљају на једном серверу, FL гарантује да подаци никада не напусте
оригинални уређај. Овај приступ је кључан за очување поверљивости и
усклађеност са регулативама за заштиту података @fl.

Систем федеративног учења функционише на принципу децентрализоване обуке.
Сваки уређај (клијент) у систему поседује локалну копију дељеног модела.
Локални модел се тренира искључиво коришћењем података који се налазе на
том уређају. Након завршетка локалног тренинга, уређај не шаље своје приватне
податке, већ само ажуриране тежине модела централном серверу.

Централни сервер има улогу агрегатора. Он прикупља ажуриране тежине од
великог броја уређаја и агрегира резултате. Израчунати, побољшани глобални
модел се затим шаље назад свим уређајима у систему, и циклус обуке се понавља.
Процес се изводи итеративно, чиме се временом побољшавају перформансе дељеног
модела, без употребе приватних података.

Потенцијалне области примене федеративног учења су индустрије које захтевају
учење модела над поверљивим скуповима података великог броја корисника, као
што су медицина, роботика и софтверско инжењерство. Најутицајнија примена
FL-а над великим скупом података је предвиђање корисничког уноса на Gboard
Android тастатури. На слици @fig:gboard приказан је примена FL за предвиђање
корисничког уноса на _Gboard Android_ тастатури @gboard.

#figure([#box(width: auto, image("../slike/gboard.jpg"));],
caption: [
  Примена FL за предвиђање корисничког уноса на _Gboard Android_ тастатури.
  Мобилни уређај локално тренира над претходним корисничким уносима и
  ажуриране тежине шаље централном серверу (A). Централни сервер агрегира
  примљене тежине (B) и шаље свим уређајима побољшан дељени модел (C) @gboard.
]
)<fig:gboard>

Кључна предност Федеративног учења је гарантована поверљивост података,
јер подаци остају искључиво на оригиналним уређајима, што омогућава децентрализовано
тренирање. Ова техника, заједно са оптимизованом диференцијалном приватношћу,
базира се на дистрибутивном машинском учењу којим се ефикасно чува поверљивост
тренинг података.

== Оптимизована диференцијална приватност

Диференцијална приватност (Differential Privacy, DP) је математички дефинисан оквир
који квантификује и гарантује приватност појединца у оквиру базе података.
Основна идеја је увођење контролисаног шума у процес обраде података,
тако да присуство или одсуство записа једног појединца у скупу података не може значајно
утицати на коначни излаз анализе или модела. Овај концепт пружа гаранцију да нападач
не може са сигурношћу закључити да ли је одређени појединац био део скупа података @dp.

Математичка дефиниција DP се најчешће изражава параметрима $ϵ$ (епсилон) и $δ$ (делта).
Алгоритам $А$ је $(ϵ, δ)$ диференцијално приватан ако за свака два скупа података
која се разликују за само један узорак $D$ и $D'$ и за сваки излазни скуп резултата $S$
важи следећа неједнакост @privacy_book:

#align(center, [
  $ P[A(D) ∈ S] ≤ e^ϵ * P[A(D') ∈ S] + δ $
])

, где је _P_ вероватноћа. Загарантовано је да се вероватноћа добијања било ког излаза
не може променити за фактор већи од $e^ϵ$ ако се један узорак дода или уклони.

Параметар ϵ одређује горњу границу промене вероватноће излаза приликом укључивања
или изостављања једног узорка података. Нижа вредност ϵ означава бољу приватност,
али потенцијално и мању тачност модела. За ϵ једнак нули излази су индентични и
добија се загарантована приватност, што је у пракси немогуће достићи.

Параметар δ представља вероватноћу да горња граница ϵ неће бити задовољена.
У идеалном теоријском случају δ тежи нули, чиме се добија диференцијална приватност
која зависи само од параметра $ϵ$, док се у практичним применама поставља на малу вредност
(нпр. $10^(-5)$), која би требало да буде мања од инверзне вредности величине скупа података
($1/"величина скупа података"$), чиме се смањује ризик угрожавања приватности појединог
 узорка у скупу података.

Оптимизована диференцијална приватност (Optimized Differential Privacy, ODP) представља
варијанту примене DP, која је специјализовану за контекст машинског учења и великих,
дистрибуираних система. Главни изазов код стандардне DP је постизање оптималног компромиса
између приватности и тачности модела. ODP користи методе попут клиповања градијента
(Gradient Clipping) @dl_dp пре додавања шума, како би се смањио потребан ниво шума
и одржала тачност модела, уз истовремено задовољавање дефинисане ϵ границе приватности.

У контексту дистрибутивног машинског учења, DP се може применити на два начина @ldp_cdp:

- Локална диференцијална приватност (Local Differential Privacy, LDP): Шум се додаје
    директно на сирове податке или ажурирања модела на самом уређају (клијенту), чиме се
    добија заштита од злонамерног централног сервера.

- Централна диференцијална приватност (Central Differential Privacy, CDP): Шум се додаје
    на централном серверу, након што су прикупљени сви доприноси клијената.
    У FL, шум се додаје на тежине модела приликом агрегације.

Технике FL и DP су комплементарне. FL пружа технички механизам за децентрализацију
тренинга и спречава прикупљање сирових података на серверу, док DP/ODP пружа математичку
гаранцију да чак ни ажурирања модела не могу открити податке појединаца. Применом ODP на
ажуриране тежине модела током процеса агрегације, спречава се да сервер може из модела
закључити осетљиве информације о појединачним тренинг подацима. Ова комбинација је неопходна
за изградњу модерних система машинског учења који су у потпуности усклађени са регулативама
за заштиту података.